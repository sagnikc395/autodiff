ðŸ§® autodiff â€” A Minimal Automatic Differentiation Engine in Python

An educational reimplementation of core automatic differentiation (autograd) concepts from first principles â€” built as part of CS689: Machine Learning at UMass Amherst (Fall 2025).

ðŸŒŸ Overview

autodiff is a lightweight Python library that implements reverse-mode automatic differentiation (the algorithm behind PyTorchâ€™s autograd and JAXâ€™s grad), written completely from scratch using NumPy only.

It constructs a computational graph dynamically during forward computation and supports efficient gradient backpropagation through complex compositions of operations â€” making it a transparent, extensible foundation for understanding how modern deep learning frameworks work under the hood.

ðŸ§  Key Features
â€¢ ðŸ”— Computation Graph Construction â€” every operation dynamically creates nodes tracking dependencies between variables.
â€¢ ðŸ”„ Reverse-Mode Backpropagation â€” gradients are computed efficiently for scalar outputs with respect to all intermediate variables.
â€¢ ðŸ§© Extensible Op Class System â€” easily define custom operations with forward and backward functions.
â€¢ ðŸ§® Matrix-Compatible â€” supports vector and matrix operations (add, matmul, logdet, solve, etc.).
â€¢ ðŸ§± Minimal Yet Modular Design â€” no external ML frameworks; pure NumPy core for educational clarity.
â€¢ ðŸ§ª Unit-Tested â€” verified gradients against PyTorch/JAX to ensure correctness.

